{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | This data was extracted from the census bureau database found at\n",
    "# | http://www.census.gov/ftp/pub/DES/www/welcome.html\n",
    "# | Donor: Ronny Kohavi and Barry Becker,\n",
    "# |        Data Mining and Visualization\n",
    "# |        Silicon Graphics.\n",
    "# |        e-mail: ronnyk@sgi.com for questions.\n",
    "# | Split into train-test using MLC++ GenCVFiles (2/3, 1/3 random).\n",
    "# | 48842 instances, mix of continuous and discrete    (train=32561, test=16281)\n",
    "# | 45222 if instances with unknown values are removed (train=30162, test=15060)\n",
    "# | Duplicate or conflicting instances : 6\n",
    "# | Class probabilities for adult.all file\n",
    "# | Probability for the label '>50K'  : 23.93% / 24.78% (without unknowns)\n",
    "# | Probability for the label '<=50K' : 76.07% / 75.22% (without unknowns)\n",
    "# |\n",
    "# | Extraction was done by Barry Becker from the 1994 Census database.  A set of\n",
    "# |   reasonably clean records was extracted using the following conditions:\n",
    "# |   ((AAGE>16) && (AGI>100) && (AFNLWGT>1)&& (HRSWK>0))\n",
    "# |\n",
    "# | Prediction task is to determine whether a person makes over 50K\n",
    "# | a year.\n",
    "# |\n",
    "# | First cited in:\n",
    "# | @inproceedings{kohavi-nbtree,\n",
    "# |    author={Ron Kohavi},\n",
    "# |    title={Scaling Up the Accuracy of Naive-Bayes Classifiers: a\n",
    "# |           Decision-Tree Hybrid},\n",
    "# |    booktitle={Proceedings of the Second International Conference on\n",
    "# |               Knowledge Discovery and Data Mining},\n",
    "# |    year = 1996,\n",
    "# |    pages={to appear}}\n",
    "# |\n",
    "# | Error Accuracy reported as follows, after removal of unknowns from\n",
    "# |    train/test sets):\n",
    "# |    C4.5       : 84.46+-0.30\n",
    "# |    Naive-Bayes: 83.88+-0.30\n",
    "# |    NBTree     : 85.90+-0.28\n",
    "# |\n",
    "# |\n",
    "# | Following algorithms were later run with the following error rates,\n",
    "# |    all after removal of unknowns and using the original train/test split.\n",
    "# |    All these numbers are straight runs using MLC++ with default values.\n",
    "# |\n",
    "# |    Algorithm               Error\n",
    "# | -- ----------------        -----\n",
    "# | 1  C4.5                    15.54\n",
    "# | 2  C4.5-auto               14.46\n",
    "# | 3  C4.5 rules              14.94\n",
    "# | 4  Voted ID3 (0.6)         15.64\n",
    "# | 5  Voted ID3 (0.8)         16.47\n",
    "# | 6  T2                      16.84\n",
    "# | 7  1R                      19.54\n",
    "# | 8  NBTree                  14.10\n",
    "# | 9  CN2                     16.00\n",
    "# | 10 HOODG                   14.82\n",
    "# | 11 FSS Naive Bayes         14.05\n",
    "# | 12 IDTM (Decision table)   14.46\n",
    "# | 13 Naive-Bayes             16.12\n",
    "# | 14 Nearest-neighbor (1)    21.42\n",
    "# | 15 Nearest-neighbor (3)    20.35\n",
    "# | 16 OC1                     15.04\n",
    "# | 17 Pebls                   Crashed.  Unknown why (bounds WERE increased)\n",
    "# |\n",
    "# | Conversion of original data as follows:\n",
    "# | 1. Discretized agrossincome into two ranges with threshold 50,000.\n",
    "# | 2. Convert U.S. to US to avoid periods.\n",
    "# | 3. Convert Unknown to \"?\"\n",
    "# | 4. Run MLC++ GenCVFiles to generate data,test.\n",
    "# |\n",
    "# | Description of fnlwgt (final weight)\n",
    "# |\n",
    "# | The weights on the CPS files are controlled to independent estimates of the\n",
    "# | civilian noninstitutional population of the US.  These are prepared monthly\n",
    "# | for us by Population Division here at the Census Bureau.  We use 3 sets of\n",
    "# | controls.\n",
    "# |  These are:\n",
    "# |          1.  A single cell estimate of the population 16+ for each state.\n",
    "# |          2.  Controls for Hispanic Origin by age and sex.\n",
    "# |          3.  Controls by Race, age and sex.\n",
    "# |\n",
    "# | We use all three sets of controls in our weighting program and \"rake\" through\n",
    "# | them 6 times so that by the end we come back to all the controls we used.\n",
    "# |\n",
    "# | The term estimate refers to population totals derived from CPS by creating\n",
    "# | \"weighted tallies\" of any specified socio-economic characteristics of the\n",
    "# | population.\n",
    "# |\n",
    "# | People with similar demographic characteristics should have\n",
    "# | similar weights.  There is one important caveat to remember\n",
    "# | about this statement.  That is that since the CPS sample is\n",
    "# | actually a collection of 51 state samples, each with its own\n",
    "# | probability of selection, the statement only applies within\n",
    "# | state.\n",
    "\n",
    "\n",
    "# >50K, <=50K.\n",
    "\n",
    "# age: continuous.\n",
    "# workclass: Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked.\n",
    "# fnlwgt: continuous.\n",
    "# education: Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool.\n",
    "# education-num: continuous.\n",
    "# marital-status: Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse.\n",
    "# occupation: Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces.\n",
    "# relationship: Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried.\n",
    "# race: White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black.\n",
    "# sex: Female, Male.\n",
    "# capital-gain: continuous.\n",
    "# capital-loss: continuous.\n",
    "# hours-per-week: continuous.\n",
    "# native-country: United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&Tobago, Peru, Hong, Holand-Netherlands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# virtualenv lrp_env\n",
    "# source lrp_env/bin/activate\n",
    "# pip install pandas\n",
    "# pip install ipykernel\n",
    "# python -m ipykernel install --user --name lrp_env --display-name \"lrp_env\"\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from keras.layers import Conv2D, Flatten, Dense, Conv1D, MaxPooling2D\n",
    "import keras.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['age', 'workclass', 'fnlwgt', 'education', 'educational-num','marital-status',\\\n",
    "               'occupation', 'relationship', 'race', 'sex','capital-gain', 'capital-loss',\\\n",
    "               'hours-per-week', 'native-country','income']\n",
    "\n",
    "original_train_df = pd.read_csv('http://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data',\\\n",
    "                 names = columns,  sep=\",\\s\")\n",
    "\n",
    "original_test_df = pd.read_csv('http://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test',\\\n",
    "                 names = columns,  sep=\",\\s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>educational-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>|1x3 Cross validator</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>Private</td>\n",
       "      <td>226802.0</td>\n",
       "      <td>11th</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>89814.0</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Farming-fishing</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>28</td>\n",
       "      <td>Local-gov</td>\n",
       "      <td>336951.0</td>\n",
       "      <td>Assoc-acdm</td>\n",
       "      <td>12.0</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Protective-serv</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>44</td>\n",
       "      <td>Private</td>\n",
       "      <td>160323.0</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>7688.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    age  workclass    fnlwgt     education  educational-num  \\\n",
       "0  |1x3 Cross validator       None       NaN          None              NaN   \n",
       "1                    25    Private  226802.0          11th              7.0   \n",
       "2                    38    Private   89814.0       HS-grad              9.0   \n",
       "3                    28  Local-gov  336951.0    Assoc-acdm             12.0   \n",
       "4                    44    Private  160323.0  Some-college             10.0   \n",
       "\n",
       "       marital-status         occupation relationship   race   sex  \\\n",
       "0                None               None         None   None  None   \n",
       "1       Never-married  Machine-op-inspct    Own-child  Black  Male   \n",
       "2  Married-civ-spouse    Farming-fishing      Husband  White  Male   \n",
       "3  Married-civ-spouse    Protective-serv      Husband  White  Male   \n",
       "4  Married-civ-spouse  Machine-op-inspct      Husband  Black  Male   \n",
       "\n",
       "   capital-gain  capital-loss  hours-per-week native-country  income  \n",
       "0           NaN           NaN             NaN           None    None  \n",
       "1           0.0           0.0            40.0  United-States  <=50K.  \n",
       "2           0.0           0.0            50.0  United-States  <=50K.  \n",
       "3           0.0           0.0            40.0  United-States   >50K.  \n",
       "4        7688.0           0.0            40.0  United-States   >50K.  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the first NaN row in the test dataset\n",
    "# original_test_df.isnull().values.any().sum()\n",
    "nan_rows = original_test_df[original_test_df['workclass'].isnull()]\n",
    "original_test_df.drop(original_test_df.index[nan_rows.index.values], inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigdata = original_train_df.append(original_test_df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_label = original_train_df['income'].copy()\n",
    "df = original_train_df.drop('income', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical = [\"workclass\",\"education\",\"marital-status\",\\\n",
    "                                          \"occupation\", \"relationship\", \"race\", \"sex\",\\\n",
    "                                           \"native-country\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# df_num = original_train_df.drop(categorical, axis = 1)\n",
    "\n",
    "# num_attribs = list(df_num)\n",
    "# cat_attribs = categorical\n",
    "\n",
    "# from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# num_pipeline = Pipeline([\n",
    "# #         ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "# #         ('attribs_adder', CombinedAttributesAdder()),\n",
    "#         ('std_scaler', StandardScaler()),\n",
    "#     ])\n",
    "\n",
    "\n",
    "# full_pipeline = ColumnTransformer([\n",
    "#         (\"num\", num_pipeline, num_attribs),\n",
    "#         (\"cat\", OneHotEncoder(), cat_attribs),\n",
    "#     ])\n",
    "\n",
    "# df_prepared = full_pipeline.fit_transform(original_train_df)\n",
    "# df_test_prepared = full_pipeline.transform(original_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_encoder = OneHotEncoder()\n",
    "data_y_prepared = cat_encoder.fit_transform(pd.DataFrame(df_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the data for a 2D convolutional neural network\n",
    "# df_prepared = df_prepared.reshape(32561, 51, 2, 1)\n",
    "# df_test_prepared = df_test_prepared.reshape(16281, 102, 1, 1)\n",
    "\n",
    "# n_features = X.shape[1]\n",
    "# n_classes = Y.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult_cat_1hot = pd.get_dummies(df[categorical])\n",
    "adult_non_cat = df.drop(categorical, axis = 1)\n",
    "adult_data_1hot = pd.concat([adult_non_cat, adult_cat_1hot], axis=1, join='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32561, 108)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adult_data_1hot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data set into training and testing\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    adult_data_1hot, data_y_prepared, test_size=0.2, random_state=42)\n",
    "\n",
    "# X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "#     adult_data_1hot, df_label, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train.astype(np.float64))\n",
    "X_test = scaler.fit_transform(X_test.astype(np.float64))\n",
    "\n",
    "\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# log_clf = LogisticRegression(solver=\"lbfgs\", random_state=42)\n",
    "# score = cross_val_score(log_clf, X_train, Y_train, cv=5, verbose=3)\n",
    "# score.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "              early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
       "              l1_ratio=0.15, learning_rate='optimal', loss='hinge',\n",
       "              max_iter=1000, n_iter_no_change=5, n_jobs=None, penalty='l2',\n",
       "              power_t=0.5, random_state=42, shuffle=True, tol=0.001,\n",
       "              validation_fraction=0.1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "sgd_clf = SGDClassifier(max_iter=1000, tol=1e-3, random_state=42)\n",
    "sgd_clf.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.83197052, 0.84579063, 0.8479613 ])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "cross_val_score(sgd_clf, X_train, Y_train, cv=3, scoring=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6513, 54, 2, 1) (26048, 54, 2, 1)\n"
     ]
    }
   ],
   "source": [
    "# X_train = X_train.values\n",
    "X_train = X_train.reshape (X_train.shape[0], 54, 2, 1)\n",
    "X_train.shape\n",
    "\n",
    "# X_test = X_test.values\n",
    "X_test = X_test.reshape (X_test.shape[0], 54, 2, 1)\n",
    "print (X_test.shape, X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 26048 samples, validate on 6513 samples\n",
      "Epoch 1/10\n",
      " - 30s - loss: 0.3370 - acc: 0.8430 - val_loss: 0.3317 - val_acc: 0.8469\n",
      "Epoch 2/10\n",
      " - 32s - loss: 0.3197 - acc: 0.8522 - val_loss: 0.3210 - val_acc: 0.8548\n",
      "Epoch 3/10\n",
      " - 30s - loss: 0.3130 - acc: 0.8540 - val_loss: 0.3105 - val_acc: 0.8603\n",
      "Epoch 4/10\n",
      " - 28s - loss: 0.3097 - acc: 0.8550 - val_loss: 0.3091 - val_acc: 0.8618\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-119-0b41ff80c95a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m history_callback = model.fit (X_train, Y_train, epochs = 10, batch_size = 10,\n\u001b[1;32m     17\u001b[0m            \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m            verbose = 2)\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/code/lrp/code/adult_dataset/lrp_env/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1176\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m     def evaluate(self,\n",
      "\u001b[0;32m~/Desktop/code/lrp/code/adult_dataset/lrp_env/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    202\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/code/lrp/code/adult_dataset/lrp_env/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2977\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2979\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2980\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2981\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/code/lrp/code/adult_dataset/lrp_env/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2935\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2936\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2937\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2938\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/code/lrp/code/adult_dataset/lrp_env/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "model = None\n",
    "model = keras.Sequential()\n",
    "\n",
    "model.add(Conv2D(64, (2,2), activation='relu', input_shape=(54,2,1)))\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(128, activation='relu'))\n",
    "\n",
    "model.add(Dense(2, input_dim = 2, activation='softmax'))  \n",
    "\n",
    "# Compile\n",
    "model.compile (loss=\"categorical_crossentropy\", optimizer = \"adam\", metrics = ['accuracy'])\n",
    "\n",
    "# Train\n",
    "history_callback = model.fit (X_train, Y_train, epochs = 10, batch_size = 10,\n",
    "           validation_data=(X_test, Y_test),\n",
    "           verbose = 2)\n",
    "\n",
    "scores = model.evaluate(X_test, Y_test, verbose=0)\n",
    "\n",
    "# Evaluate the model's performance:\n",
    "print('Test log loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lrp_env",
   "language": "python",
   "name": "lrp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
