This project is an ongoing attempt to research and implement techniques that make the outcome of neural networks more explainable. The ultimate goal of this project is to empower decision-makers to interpret the results of deep-learning algorithms and explain the basis of their decisions. More specifically, this project will implement the Layerwise Relevance Propagation technique (https://github.com/albermax/innvestigate) on deep neural network models. Using K-Means Clustering, then, it will capture the hidden patterns in the LRP data to achieve a level of interpretability - I train a kNN classifier model on the same dataset, but this time using the LRP values as a factor. The goal is to see to what degree the predictions of this kNN model correspond to those of the DNN. I short, I'm hoping to see how we can add a degree of explainability to what happens under the hood in a neural network. The results can be used to solve a variety of real-life problems by assisting professionals in healthcare and finance industries.
